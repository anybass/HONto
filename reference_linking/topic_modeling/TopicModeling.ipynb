{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TopicModeling.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ypGjoQ8Wfh5H",
        "W9BbAC8bfvkp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L780NFYmQ_tg",
        "colab_type": "text"
      },
      "source": [
        "**Topic Modeling for German Legal Text**\n",
        "\n",
        "In this notebook we demonstrate the use of Gensim, for basic topic modeling of German Legal Text. A similar approach serves as a backbone of our study, \"[ERST: Leveraging Topic Features for Context-Aware Legal Reference Linking](https://https://jurix2019.oeg-upm.net/)\" \n",
        "\n",
        "We provide a resources folder, with supporting content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypGjoQ8Wfh5H",
        "colab_type": "text"
      },
      "source": [
        "# Initialization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ddqaHUHQ-Ke",
        "colab_type": "code",
        "outputId": "09012320-bfdf-4abe-cab0-0732fa1b0789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        }
      },
      "source": [
        "!pip install nltk germalemma joblib pyLDAvis gensim\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: germalemma in /usr/local/lib/python3.6/dist-packages (0.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (0.14.0)\n",
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: Pyphen>=0.9.5 in /usr/local/lib/python3.6/dist-packages (from germalemma) (0.9.5)\n",
            "Requirement already satisfied: PatternLite>=3.6 in /usr/local/lib/python3.6/dist-packages (from germalemma) (3.6)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.25.3)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.17.4)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.10.3)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.14)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.33.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.0)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.3.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (42.0.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.0.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.3.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.10.32)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.32 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.13.32)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dI876UuXrUL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "9ab0fcf4-2a10-4725-9616-01020dbfd0cf"
      },
      "source": [
        "import nltk.data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB1mkcFlR0K3",
        "colab_type": "text"
      },
      "source": [
        "At first we must define stopwords to be removed, and import relevant libraries. We give examples of some stopwords we remove. This needs to be carefully adapted to the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI6A4OV5Q--o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from germalemma import GermaLemma\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "from pattern.de import parse, split\n",
        "import re\n",
        "from joblib import Parallel, delayed \n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('german')\n",
        "stopwords.append(\"the\")\n",
        "stopwords.append(\"of\")\n",
        "stopwords.append(\"and\")\n",
        "stopwords.append(\"http\")\n",
        "stopwords.append(\"https\")\n",
        "stopwords.append(\"their\")\n",
        "stopwords.append(\"werden.en\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9BbAC8bfvkp",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbFHFNS_T4XZ",
        "colab_type": "text"
      },
      "source": [
        "Now we will start going through a series of documents inside a folder, saving for each, their content and the filename.\n",
        "\n",
        "Please adapt the *path* variable, to suit your data location.\n",
        "\n",
        "On some cases the encoding might also need adaptation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8ZTLahCUWaw",
        "colab_type": "code",
        "outputId": "3b919885-c7e4-44f6-bc04-629812753ad3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "path=\"examples\"\n",
        "docs=[]\n",
        "filenames=[]\n",
        "\n",
        "#We start by going through the path, and reading documents\n",
        "for r, d, f in os.walk(path):\n",
        "  for n in f:\n",
        "    fi =open(path+\"/\"+n, 'r', encoding='utf-8')\n",
        "    filenames.append(n)\n",
        "    x = fi.read()\n",
        "    fi.close()\n",
        "    docs.append(x)\n",
        "\n",
        "print(\"Files found:\\n\"+','.join(sorted(filenames)))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files found:\n",
            "example.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvaE087UWDWk",
        "colab_type": "text"
      },
      "source": [
        "We parallelize the tokenization, for a small performance improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaWdr3_JXRO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "abee3210-bc04-4c71-8ef6-c52247ea8fdf"
      },
      "source": [
        "def tokenize(d):\n",
        "  result=list(tokenizer.tokenize(d))\n",
        "  return result\n",
        "\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\n",
        "doc_sentences=Parallel(n_jobs=10)(delayed(tokenize)(d) for d in docs)\n",
        "print(\"Done with tokenization\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done with tokenization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo600R-CTSTY",
        "colab_type": "text"
      },
      "source": [
        "*Note:* We use a classifier-based POS tagger, pickled and provided as a linked resource."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyR7kB25TUBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ClassifierBasedGermanTagger import ClassifierBasedGermanTagger #Provided in the resource folder\n",
        "with open('nltk_german_classifier_data.pickle', 'rb') as f:\n",
        "    tagger = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAhVQ1ozbD1n",
        "colab_type": "text"
      },
      "source": [
        "Next we also illustrate a parallelized version of tagging. Here timeouts can be defined and the behavior to deal with them.\n",
        "\n",
        "At this stage there are options for removing characters that would create inaccurate sentence-level splits.\n",
        "\n",
        "The result of the tagging will look something like: \n",
        "*('Prospekthaftung', 'NE'), ('und', 'KON')...('geschlossenen', 'ADJA'), ('Fonds.', 'NN')...*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDjQEym2ar8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "54b7181e-e764-43bf-8f9f-b224ca24384d"
      },
      "source": [
        "verbose=False\n",
        "timeout_duration=10\n",
        "default=None #default result when a timeout takes place\n",
        "\n",
        "def timeout(func, args=(), kwargs={}, timeout_duration=1, default=None):\n",
        "    import signal\n",
        "    class TimeoutError(Exception):\n",
        "        pass\n",
        "    def handler(signum, frame):\n",
        "        raise TimeoutError()\n",
        "\n",
        "    # set the timeout handler\n",
        "    signal.signal(signal.SIGALRM, handler) \n",
        "    signal.alarm(timeout_duration)\n",
        "    try:\n",
        "        result = func(args)\n",
        "    except TimeoutError as exc:\n",
        "        print(\"Timeout!\")\n",
        "        result = default\n",
        "    finally:\n",
        "        signal.alarm(0)\n",
        "    return result\n",
        "\n",
        "def tag_this(doc,number,total):\n",
        "  tags_doc=list()\n",
        "  s_count=0\n",
        "  for sentence in doc:\n",
        "    s_count+=1\n",
        "    if (verbose and s_count%50==0):\n",
        "      print(str(s_count)+\" of \"+str(len(doc))+\"; doc #\"+str(number)+\" of \"+str(total))\n",
        "    #We illustrate in the next line the replacement of common sentence-level split characters that might cause inaccurate results.\n",
        "    split_sentence=[x.replace(\"]\", \"\").replace(\"[\", \"\").replace(\"^\", \"\").replace(\",\", \"\").replace(\":\", \"\").replace(\";\", \"\") for x in str(sentence).replace(\"\\\\n\", \" \").replace(\"\\n\", \" \").replace(\"„\",\"„ \").replace(\"/\", \" \").replace(\"“\",\" “\").replace(\"(\",\" (\").replace(\")\",\") \").replace(\"?\",\" ? \").replace(\"…\",\" …\").split(\" \")]\n",
        "    result=timeout(tagger.tag, args=(split_sentence), kwargs={},timeout_duration=timeout_duration, default=None)  \n",
        "    if result!= default:#Special handing can be added here...\n",
        "      tags_doc.append(result)\n",
        "  return(tags_doc)\n",
        "\n",
        "#Now we do part of speech tagging\n",
        "tagged=Parallel(n_jobs=5)(delayed(tag_this)(doc_sentences[i],i,len(doc_sentences)) for i in range(0,len(doc_sentences)))\n",
        "print(\"Done\")\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrdTAD88cqpc",
        "colab_type": "text"
      },
      "source": [
        "After tagging we can carry out lemmatization.\n",
        "\n",
        "The result will look something like: \n",
        "*'Prospekthaftung', 'und',..., 'geschlossen','Fonds.'...*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjWEb9jNc_qp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dab11778-40de-41a8-88c8-a5ef4fc4c86c"
      },
      "source": [
        "lemmatizer=GermaLemma()\n",
        "lemmatized=[]\n",
        "for doc in tagged:\n",
        "  lemmas_doc=list()\n",
        "  for sentence in doc:\n",
        "    for item in sentence:\n",
        "      if len(item[0])>2 and not item[0].lower() in stopwords and not re.match(\".{1,3}\\.\", item[0].lower()) and not re.match(\"[0-9]{1}\", item[0].lower()) and not re.match(\"\\W{1}\", item[0].lower()) and not re.match(\"^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\", item[0].upper()):\n",
        "        if not str(item[1]).startswith((\"N\", \"V\", \"ADJ\", \"ADV\")):\n",
        "          lemmat_word=item[0].lower().strip('\\\"').strip('„').strip(\"“\")\n",
        "          if lemmat_word[-1] == '.':\n",
        "            lemmat_word=lemmat_word[0:-1]\n",
        "          lemmat_word=lemmat_word.strip(')').strip('(')\n",
        "          lemmas_doc.append(lemmat_word)\n",
        "        else:\n",
        "          passes=False\n",
        "          try:\n",
        "            lemmat_word=str(item[0].lower().strip('\\\"').strip('„').strip(\"“\"))\n",
        "            if lemmat_word[-1] == '.':\n",
        "              lemmat_word=lemmat_word[0:-1]\n",
        "            lemmat_word=lemmat_word.strip(')').strip('(')\n",
        "            lemmatized_word = lemmatizer.find_lemma(lemmat_word, str(item[1]))\n",
        "            if not lemmatized_word in stopwords:\n",
        "              passes=True\n",
        "          except:\n",
        "            print(str(item)+ \", gave an error!\")\n",
        "          if passes:\n",
        "            lemmas_doc.append(lemmatized_word.lower())\n",
        "  lemmatized.append(lemmas_doc)\n",
        "print(\"Done\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeSz8urifZxG",
        "colab_type": "text"
      },
      "source": [
        "Next we save in a CSV file, such that each line is a document. This concludes the pre-processing state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI8X9sQ_fXVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(\"corpus_lemmatized.csv\", \"w\")\n",
        "for doc in lemmatized:\n",
        "  str_acc=\"\\\"\"\n",
        "  for item in doc:\n",
        "    str_acc+=item+\" \"\n",
        "  str_acc+=\"\\\"\"\n",
        "  f.write(str_acc+\"\\n\")\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcI2j2CQf8lF",
        "colab_type": "text"
      },
      "source": [
        "# Topic Modeling with Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZqyqKg7gAqP",
        "colab_type": "text"
      },
      "source": [
        "In this section we illustrate the process of topic modeling with [Gensim](https://https://radimrehurek.com/gensim/). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP_P_ZuTgJyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "463BhQ-egeV4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "84da14f0-6695-41eb-a083-cd336526b16c"
      },
      "source": [
        "#Lemmatized represents the csv generated during pre-processing\n",
        "\n",
        "# Create dictionary and corpus\n",
        "id2word = corpora.Dictionary(lemmatized)\n",
        "corpus = [id2word.doc2bow(text) for text in lemmatized]\n",
        "\n",
        "lda_model = LdaMulticore(corpus=corpus,id2word=id2word,\n",
        "                          num_topics=30, random_state=100,\n",
        "                          #update_every=1,\n",
        "                          chunksize=100, passes=10, #alpha='auto',\n",
        "\t\t\t\t\t                eta='auto', iterations=200, per_word_topics=True,workers=10)\n",
        "\n",
        "\n",
        "print(\"Done\")\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. The lower the better.\n",
        "\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "\n",
            "Perplexity:  -5.560813559727236\n",
            "\n",
            "Coherence Score:  0.9999999999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBao3mxqiRbK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "cfae875f-6fff-420c-e78e-d40a14f83b58"
      },
      "source": [
        "import pickle\n",
        "pickle.dump(lda_model, open( \"lda_model.p\", 'wb') )\n",
        "print(\"Dumped model to file: lda_model.p\")\n",
        "\n",
        "pickle.dump(id2word, open( \"id2word.p\", 'wb' ) )\n",
        "print(\"Dumped id2word to file: id2word.p\")\n",
        "\n",
        "pickle.dump(corpus, open( \"corpus.p\", 'wb' ) )\n",
        "print(\"Dumped corpus to file: corpus.p\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dumped model to file: lda_model.p\n",
            "Dumped id2word to file: id2word.p\n",
            "Dumped corpus to file: corpus.p\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2HfOKZIjO50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('lda_model.p', 'rb') as f:\n",
        "    lda_model = pickle.load(f)\n",
        "\n",
        "with open('id2word.p', 'rb') as f:\n",
        "    id2word = pickle.load(f)\n",
        "\n",
        "with open('corpus.p', 'rb') as f:\n",
        "    corpus = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJUEifkNkD0c",
        "colab_type": "text"
      },
      "source": [
        "Next we use the LDA visualization library ([LDAVis](https://https://github.com/cpsievert/LDAvis), [PyLDAVis](https://https://github.com/bmabey/pyLDAvis)) for some simple visualization of the topics and key words.\n",
        "\n",
        "*Please note:*\n",
        "\n",
        "\n",
        "*   Deprecation warnings might show.\n",
        "*   With the simple example provided, it is possible to stumble upon an error related to missing data, that would produce complex data types that cannot be serialized to the JSON format used by the LDAVis library.\n",
        "*   Running this notebook on different congfigurations from which the pickled data was produced might lead to errors with data types.\n",
        "*   We share example pickled files from our study, [here](https://https://drive.google.com/open?id=1RUTGL_kx3oW4kT-gbV5zrVX3b7KBCMwp), and we include as resources the generated topic visualizations for our dataset.\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk24EN_dkESA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualisation = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "pyLDAvis.save_html(visualisation, \"visualization.html\")\n",
        "print(\"Done\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlLNHfOSsw2K",
        "colab_type": "text"
      },
      "source": [
        "For topic modeling with a worklow similar to the one described in this notebook, but using new datasets, many other studies would be required (e.g. as suggested [here](https://github.com/trinker/topicmodels_learning))."
      ]
    }
  ]
}